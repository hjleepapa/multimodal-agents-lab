{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM8rg08YhqZe"
      },
      "source": [
        "# Step 1: Setup prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pymongo import MongoClient\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oXLWCWEghuOX"
      },
      "outputs": [
        {
          "ename": "ServerSelectionTimeoutError",
          "evalue": "localhost:27017: [Errno 61] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 68d3b78570003c5c32f93e8d, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 61] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mServerSelectionTimeoutError\u001b[39m               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m mongodb_client = MongoClient(MONGODB_URI)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Check the connection to the server\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mmongodb_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43madmin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mping\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/AI_Agent/multimodal/multimodal-agents-lab/.venv/lib/python3.13/site-packages/pymongo/_csot.py:119\u001b[39m, in \u001b[36mapply.<locals>.csot_wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[32m    118\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/AI_Agent/multimodal/multimodal-agents-lab/.venv/lib/python3.13/site-packages/pymongo/synchronous/database.py:926\u001b[39m, in \u001b[36mDatabase.command\u001b[39m\u001b[34m(self, command, value, check, allowable_errors, read_preference, codec_options, session, comment, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read_preference \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    925\u001b[39m     read_preference = (session \u001b[38;5;129;01mand\u001b[39;00m session._txn_read_preference()) \u001b[38;5;129;01mor\u001b[39;00m ReadPreference.PRIMARY\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_conn_for_reads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommand_name\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[32m    927\u001b[39m     connection,\n\u001b[32m    928\u001b[39m     read_preference,\n\u001b[32m    929\u001b[39m ):\n\u001b[32m    930\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._command(\n\u001b[32m    931\u001b[39m         connection,\n\u001b[32m    932\u001b[39m         command,\n\u001b[32m   (...)\u001b[39m\u001b[32m    939\u001b[39m         **kwargs,\n\u001b[32m    940\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/AI_Agent/multimodal/multimodal-agents-lab/.venv/lib/python3.13/site-packages/pymongo/synchronous/mongo_client.py:1699\u001b[39m, in \u001b[36mMongoClient._conn_for_reads\u001b[39m\u001b[34m(self, read_preference, session, operation)\u001b[39m\n\u001b[32m   1692\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_conn_for_reads\u001b[39m(\n\u001b[32m   1693\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1694\u001b[39m     read_preference: _ServerMode,\n\u001b[32m   1695\u001b[39m     session: Optional[ClientSession],\n\u001b[32m   1696\u001b[39m     operation: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   1697\u001b[39m ) -> ContextManager[\u001b[38;5;28mtuple\u001b[39m[Connection, _ServerMode]]:\n\u001b[32m   1698\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m read_preference \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mread_preference must not be None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1699\u001b[39m     server = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1700\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._conn_from_server(read_preference, server, session)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/AI_Agent/multimodal/multimodal-agents-lab/.venv/lib/python3.13/site-packages/pymongo/synchronous/mongo_client.py:1647\u001b[39m, in \u001b[36mMongoClient._select_server\u001b[39m\u001b[34m(self, server_selector, session, operation, address, deprioritized_servers, operation_id)\u001b[39m\n\u001b[32m   1645\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m AutoReconnect(\u001b[33m\"\u001b[39m\u001b[33mserver \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m no longer available\u001b[39m\u001b[33m\"\u001b[39m % address)  \u001b[38;5;66;03m# noqa: UP031\u001b[39;00m\n\u001b[32m   1646\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1647\u001b[39m         server = \u001b[43mtopology\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1648\u001b[39m \u001b[43m            \u001b[49m\u001b[43mserver_selector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[43m            \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1650\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdeprioritized_servers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeprioritized_servers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1651\u001b[39m \u001b[43m            \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1652\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1653\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m server\n\u001b[32m   1654\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PyMongoError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# Server selection errors in a transaction are transient.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/AI_Agent/multimodal/multimodal-agents-lab/.venv/lib/python3.13/site-packages/pymongo/synchronous/topology.py:401\u001b[39m, in \u001b[36mTopology.select_server\u001b[39m\u001b[34m(self, selector, operation, server_selection_timeout, address, deprioritized_servers, operation_id)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect_server\u001b[39m(\n\u001b[32m    392\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    393\u001b[39m     selector: Callable[[Selection], Selection],\n\u001b[32m   (...)\u001b[39m\u001b[32m    398\u001b[39m     operation_id: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    399\u001b[39m ) -> Server:\n\u001b[32m    400\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Like select_servers, but choose a random server if several match.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     server = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_selection_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeprioritized_servers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _csot.get_timeout():\n\u001b[32m    410\u001b[39m         _csot.set_rtt(server.description.min_round_trip_time)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/AI_Agent/multimodal/multimodal-agents-lab/.venv/lib/python3.13/site-packages/pymongo/synchronous/topology.py:379\u001b[39m, in \u001b[36mTopology._select_server\u001b[39m\u001b[34m(self, selector, operation, server_selection_timeout, address, deprioritized_servers, operation_id)\u001b[39m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_select_server\u001b[39m(\n\u001b[32m    371\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    372\u001b[39m     selector: Callable[[Selection], Selection],\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m     operation_id: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    378\u001b[39m ) -> Server:\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     servers = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mselect_servers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_selection_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_id\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m     servers = _filter_servers(servers, deprioritized_servers)\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(servers) == \u001b[32m1\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/AI_Agent/multimodal/multimodal-agents-lab/.venv/lib/python3.13/site-packages/pymongo/synchronous/topology.py:286\u001b[39m, in \u001b[36mTopology.select_servers\u001b[39m\u001b[34m(self, selector, operation, server_selection_timeout, address, operation_id)\u001b[39m\n\u001b[32m    283\u001b[39m     server_timeout = server_selection_timeout\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     server_descriptions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select_servers_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    291\u001b[39m         cast(Server, \u001b[38;5;28mself\u001b[39m.get_server_by_address(sd.address)) \u001b[38;5;28;01mfor\u001b[39;00m sd \u001b[38;5;129;01min\u001b[39;00m server_descriptions\n\u001b[32m    292\u001b[39m     ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/AI_Agent/multimodal/multimodal-agents-lab/.venv/lib/python3.13/site-packages/pymongo/synchronous/topology.py:336\u001b[39m, in \u001b[36mTopology._select_servers_loop\u001b[39m\u001b[34m(self, selector, timeout, operation, operation_id, address)\u001b[39m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _SERVER_SELECTION_LOGGER.isEnabledFor(logging.DEBUG):\n\u001b[32m    326\u001b[39m         _debug_log(\n\u001b[32m    327\u001b[39m             _SERVER_SELECTION_LOGGER,\n\u001b[32m    328\u001b[39m             message=_ServerSelectionStatusMessage.FAILED,\n\u001b[32m   (...)\u001b[39m\u001b[32m    334\u001b[39m             failure=\u001b[38;5;28mself\u001b[39m._error_message(selector),\n\u001b[32m    335\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServerSelectionTimeoutError(\n\u001b[32m    337\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._error_message(selector)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Timeout: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms, Topology Description: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.description\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    338\u001b[39m     )\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logged_waiting:\n\u001b[32m    341\u001b[39m     _debug_log(\n\u001b[32m    342\u001b[39m         _SERVER_SELECTION_LOGGER,\n\u001b[32m    343\u001b[39m         message=_ServerSelectionStatusMessage.WAITING,\n\u001b[32m   (...)\u001b[39m\u001b[32m    349\u001b[39m         remainingTimeMS=\u001b[38;5;28mint\u001b[39m(end_time - time.monotonic()),\n\u001b[32m    350\u001b[39m     )\n",
            "\u001b[31mServerSelectionTimeoutError\u001b[39m: localhost:27017: [Errno 61] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 68d3b78570003c5c32f93e8d, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 61] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>"
          ]
        }
      ],
      "source": [
        "# If you are using your own MongoDB Atlas cluster, use the connection string for your cluster here\n",
        "MONGODB_URI = os.getenv(\"MONGODB_URI\")\n",
        "# Initialize a MongoDB Python client\n",
        "mongodb_client = MongoClient(MONGODB_URI)\n",
        "# Check the connection to the server\n",
        "mongodb_client.admin.command(\"ping\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtain a Gemini API key in [Google AI Studio](https://aistudio.google.com/app/apikey)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = \"your-google-api-key\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUf3jtFzO4-V"
      },
      "source": [
        "# Step 2: Read PDF from URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pymupdf\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "📚 https://pymupdf.readthedocs.io/en/latest/how-to-open-a-file.html#opening-remote-files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the DeepSeek paper\n",
        "response = requests.get(\"https://arxiv.org/pdf/2501.12948\")\n",
        "if response.status_code != 200:\n",
        "    raise ValueError(f\"Failed to download PDF. Status code: {response.status_code}\")\n",
        "# Get the content of the response\n",
        "pdf_stream = response.content\n",
        "# Open the data in `pdf_stream` as a PDF document and store it in `pdf`.\n",
        "# HINT: Set the `filetype` argument to \"pdf\".\n",
        "pdf = <CODE_BLOCK_1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3: Store PDF images locally and extract metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "📚 https://pymupdf.readthedocs.io/en/latest/page.html#Page.get_pixmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "zoom = 3.0\n",
        "# Set image matrix dimensions\n",
        "mat = pymupdf.Matrix(zoom, zoom)\n",
        "# Iterate through the pages of the PDF\n",
        "for n in tqdm(range(pdf.page_count)):\n",
        "    temp = {}\n",
        "    # Use the `get_pixmap` method to render the PDF page as a matrix of pixels as specified by the variable `mat`\n",
        "    # HINT: Access the PDF page as pdf[n]\n",
        "    pix = <CODE_BLOCK_2>\n",
        "    # Store image locally\n",
        "    key = f\"data/images/{n+1}.png\"\n",
        "    pix.save(key)\n",
        "    # Extract image metadata to be stored in MongoDB\n",
        "    temp[\"key\"] = key\n",
        "    temp[\"width\"] = pix.width\n",
        "    temp[\"height\"] = pix.height\n",
        "    docs.append(temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 4: Generate image embeddings\n",
        "\n",
        "Uncomment this section only if you are generating embedding using your own Voyage AI API key.\n",
        "\n",
        "Follow the steps [here](https://docs.voyageai.com/docs/api-key-and-installation#authentication-with-api-keys) to obtain a Voyage AI API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from voyageai import Client\n",
        "# from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Set Voyage AI API Key\n",
        "# os.environ[\"VOYAGE_API_KEY\"] = \"your-voyage-api-key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# voyageai_client = Client()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def get_embedding(data, input_type):\n",
        "#     \"\"\"\n",
        "#     Get Voyage AI embeddings for images and text.\n",
        "\n",
        "#     Args:\n",
        "#         data: An image or text to embed\n",
        "#         input_type: Input type, either \"document\" or \"query\"\n",
        "\n",
        "#     Returns: Embeddings as a list\n",
        "#     \"\"\"\n",
        "#     embedding = voyageai_client.multimodal_embed(\n",
        "#         inputs=[[data]], model=\"voyage-multimodal-3\", input_type=input_type\n",
        "#     ).embeddings[0]\n",
        "#     return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# embedded_docs = []\n",
        "# for doc in tqdm(docs):\n",
        "#     # Open the image from file\n",
        "#     img = Image.open(f\"{doc['key']}\")\n",
        "#     # Add the embeddings to the document\n",
        "#     doc[\"embedding\"] = get_embedding(img, \"document\")\n",
        "#     embedded_docs.append(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 5: Write embeddings and metadata to MongoDB\n",
        "\n",
        "In this step, we are ingesting a dataset with multimodal embeddings pre-generated, into MongoDB. \n",
        "\n",
        "If you would like to understand how to the embedding process works, uncomment and work through the code in Step 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Database name\n",
        "DB_NAME = \"mongodb_aiewf\"\n",
        "# Name of the collection to insert documents into\n",
        "COLLECTION_NAME = \"multimodal_workshop\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to the collection\n",
        "collection = mongodb_client[DB_NAME][COLLECTION_NAME]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read data from local file\n",
        "with open(\"data/embeddings.json\", \"r\") as data_file:\n",
        "    json_data = data_file.read()\n",
        "data = json.loads(json_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.insert_many"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete existing documents from the `collection` collection\n",
        "collection.delete_many({})\n",
        "print(f\"Deleted existing documents from the {COLLECTION_NAME} collection.\")\n",
        "# Bulk insert documents in `data`, into the `collection` collection.\n",
        "<CODE_BLOCK_3>\n",
        "print(\n",
        "    f\"{collection.count_documents({})} documents ingested into the {COLLECTION_NAME} collection.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 6: Create a vector search index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "VS_INDEX_NAME = \"vector_index\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create vector index definition specifying:\n",
        "# path: Path to the embeddings field\n",
        "# numDimensions: Number of embedding dimensions- depends on the embedding model used\n",
        "# similarity: Similarity metric. One of cosine, euclidean, dotProduct.\n",
        "model = {\n",
        "    \"name\": VS_INDEX_NAME,\n",
        "    \"type\": \"vectorSearch\",\n",
        "    \"definition\": {\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"type\": \"vector\",\n",
        "                \"path\": \"embedding\",\n",
        "                \"numDimensions\": 1024,\n",
        "                \"similarity\": \"cosine\",\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_search_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a vector search index with the above `model` for the `collection` collection\n",
        "<CODE_BLOCK_4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify that the index is in READY status before proceeding\n",
        "list(collection.list_search_indexes())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZfheX5FiIhU"
      },
      "source": [
        "# Step 7: Create agent tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "📚 https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples (Refer to Basic Example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_information_for_question_answering(user_query: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Retrieve information using vector search to answer a user query.\n",
        "\n",
        "    Args:\n",
        "    user_query (str): The user's query string.\n",
        "\n",
        "    Returns:\n",
        "    str: The retrieved information formatted as a string.\n",
        "    \"\"\"\n",
        "    # Embed the user query using our serverless endpoint\n",
        "    response = requests.post(\n",
        "        url=SERVERLESS_URL,\n",
        "        json={\n",
        "            \"task\": \"get_embedding\",\n",
        "            \"data\": {\"input\": user_query, \"input_type\": \"query\"},\n",
        "        },\n",
        "    )\n",
        "    # Extract the embedding from the response\n",
        "    query_embedding = response.json()[\"embedding\"]\n",
        "\n",
        "    # Define an aggregation pipeline consisting of a $vectorSearch stage followed by a $project stage\n",
        "    # Set the number of candidates to 150 and only return the top 2 documents from the vector search\n",
        "    # In the $project stage, exclude the `_id` field, include these fields: `key`, `width`, `height`, and the `vectorSearchScore`\n",
        "    # NOTE: Use variables defined previously for the `index`, `queryVector` and `path` fields in the $vectorSearch stage\n",
        "    pipeline = <CODE_BLOCK_5>\n",
        "\n",
        "    # Execute the aggregation `pipeline` against the `collection` collection and store the results in `results`\n",
        "    results = <CODE_BLOCK_6>\n",
        "    # Get images from local storage\n",
        "    keys = [result[\"key\"] for result in results]\n",
        "    print(f\"Keys: {keys}\")\n",
        "    return keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "📚 https://ai.google.dev/gemini-api/docs/function-calling?example=meeting#step_1_define_function_declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the function declaration for the `get_information_for_question_answering` function\n",
        "get_information_for_question_answering_declaration = {\n",
        "    \"name\": <CODE_BLOCK_7>,\n",
        "    \"description\": \"Retrieve information using vector search to answer a user query.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"user_query\": {\n",
        "                \"type\": <CODE_BLOCK_8>,\n",
        "                \"description\": \"Query string to use for vector search\",\n",
        "            }\n",
        "        },\n",
        "        \"required\": <CODE_BLOCK_9>,\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 8: Instantiate the Gemini client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LLM = \"gemini-2.0-flash\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gemini_client = genai.Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 9: Create generation config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a generation config with the `get_information_for_question_answering_declaration` function declaration and `temperature` set to 0.0\n",
        "tools = types.Tool(\n",
        "    function_declarations=[get_information_for_question_answering_declaration]\n",
        ")\n",
        "tools_config = types.GenerateContentConfig(tools=[tools], temperature=0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 10: Define a function for tool selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.genai.types import FunctionCall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "📚 https://ai.google.dev/gemini-api/docs/function-calling?example=meeting#step_4_create_user_friendly_response_with_function_result_and_call_the_model_again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_tool(messages: List) -> FunctionCall | None:\n",
        "    \"\"\"\n",
        "    Use an LLM to decide which tool to call\n",
        "\n",
        "    Args:\n",
        "        messages (List): Messages as a list\n",
        "\n",
        "    Returns:\n",
        "        functionCall: Function call object consisting of the tool name and arguments\n",
        "    \"\"\"\n",
        "    system_prompt = [\n",
        "        (\n",
        "            \"You're an AI assistant. Based on the given information, decide which tool to use.\"\n",
        "            \"If the user is asking to explain an image, don't call any tools unless that would help you better explain the image.\"\n",
        "            \"Here is the provided information:\\n\"\n",
        "        )\n",
        "    ]\n",
        "    # Input to the LLM\n",
        "    contents = system_prompt + messages\n",
        "    # Use the `gemini_client`, `LLM`, `contents` and `tools_config` defined previously to generate a response using Gemini\n",
        "    response = <CODE_BLOCK_10>\n",
        "    # Extract and return the function call from the response\n",
        "    return response.candidates[0].content.parts[0].function_call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 10: Define a function to execute tools and generate responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "📚 https://ai.google.dev/gemini-api/docs/function-calling?example=meeting#step_3_execute_set_light_values_function_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_answer(user_query: str, images: List = []) -> str:\n",
        "    \"\"\"\n",
        "    Execute any tools and generate a response\n",
        "\n",
        "    Args:\n",
        "        user_query (str): User's query string\n",
        "        images (List): List of filepaths. Defaults to [].\n",
        "\n",
        "    Returns:\n",
        "        str: LLM-generated response\n",
        "    \"\"\"\n",
        "    # Use the `select_tool` function above to get the tool config\n",
        "    # NOTE: Input to `select_tool` should be a list\n",
        "    tool_call = <CODE_BLOCK_11>\n",
        "    # If a tool call is found and the name is `get_information_for_question_answering`\n",
        "    if (\n",
        "        tool_call is not None\n",
        "        and tool_call.name == \"get_information_for_question_answering\"\n",
        "    ):\n",
        "        print(f\"Agent: Calling tool: {tool_call.name}\")\n",
        "        # Call the tool with the arguments extracted by the LLM\n",
        "        tool_images = <CODE_BLOCK_12>\n",
        "        # Add images return by the tool to the list of input images if any\n",
        "        images.extend(tool_images)\n",
        "\n",
        "\n",
        "    system_prompt = f\"Answer the questions based on the provided context only. If the context is not sufficient, say I DON'T KNOW. DO NOT use any other information to answer the question.\"\n",
        "    # Pass the system prompt, user query, and content retrieved using vector search (`images`) as input to the LLM\n",
        "    contents = [system_prompt] + [user_query] + [Image.open(image) for image in images]\n",
        "\n",
        "    # Get the response from the LLM\n",
        "    response = gemini_client.models.generate_content(\n",
        "        model=LLM,\n",
        "        contents=contents,\n",
        "        config=types.GenerateContentConfig(temperature=0.0),\n",
        "    )\n",
        "    answer = response.text\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 11: Define a function to execute the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_agent(user_query: str, images: List = []) -> None:\n",
        "    \"\"\"\n",
        "    Execute the agent.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): User query\n",
        "        images (List, optional): List of filepaths. Defaults to [].\n",
        "    \"\"\"\n",
        "    response = generate_answer(user_query, images)\n",
        "    print(\"Agent:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the agent with a text input\n",
        "execute_agent(\"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the agent with an image input\n",
        "execute_agent(\"Explain the graph in this image:\", [\"data/test.png\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 12: Add memory to the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the history collection\n",
        "history_collection = mongodb_client[DB_NAME][\"history\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an index on `session_id` on the `history_collection` collection\n",
        "<CODE_BLOCK_13>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.insert_one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def store_chat_message(session_id: str, role: str, type: str, content: str) -> None:\n",
        "    \"\"\"\n",
        "    Create chat history document and store it in MongoDB\n",
        "\n",
        "    Args:\n",
        "        session_id (str): Session ID\n",
        "        role (str): Message role, one of `human` or `agent`.\n",
        "        type (str): Type of message, one of `text` or `image`.\n",
        "        content (str): Content of the message. For images, this is the image key.\n",
        "    \"\"\"\n",
        "    # Create a message object with `session_id`, `role`, `type`, `content` and `timestamp` fields\n",
        "    # `timestamp` should be set the current timestamp\n",
        "    message = {\n",
        "        \"session_id\": session_id,\n",
        "        \"role\": role,\n",
        "        \"type\": type,\n",
        "        \"content\": content,\n",
        "        \"timestamp\": datetime.now(),\n",
        "    }\n",
        "    # Insert the `message` into the `history_collection` collection\n",
        "    <CODE_BLOCK_14>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/cursor.html#pymongo.cursor.Cursor.sort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_session_history(session_id: str) -> List:\n",
        "    \"\"\"\n",
        "    Retrieve chat history for a particular session.\n",
        "\n",
        "    Args:\n",
        "        session_id (str): Session ID\n",
        "\n",
        "    Returns:\n",
        "        List: List of messages. Can be a combination of text and images.\n",
        "    \"\"\"\n",
        "    # Query the `history_collection` collection for documents where the \"session_id\" field has the value of the input `session_id`\n",
        "    # Sort the results in increasing order of the values in `timestamp` field\n",
        "    cursor = <CODE_BLOCK_15>\n",
        "    messages = []\n",
        "    if cursor:\n",
        "        for msg in cursor:\n",
        "            # Is the message type is `text`, append the content as is\n",
        "            if msg[\"type\"] == \"text\":\n",
        "                messages.append(msg[\"content\"])\n",
        "            # If message type is `image`, open the image\n",
        "            elif msg[\"type\"] == \"image\":\n",
        "                messages.append(Image.open(msg[\"content\"]))\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_answer(session_id: str, user_query: str, images: List = []) -> str:\n",
        "    \"\"\"\n",
        "    Execute any tools and generate a response\n",
        "\n",
        "    Args:\n",
        "        session_id (str): Session ID\n",
        "        user_query (str): User's query string\n",
        "        images (List): List of filepaths. Defaults to [].\n",
        "\n",
        "    Returns:\n",
        "        str: LLM-generated response\n",
        "    \"\"\"\n",
        "    # Retrieve past conversation history for the specified `session_id` using the `retrieve_session_history` method\n",
        "    history = <CODE_BLOCK_16>\n",
        "    # Determine if any additional tools need to be called\n",
        "    tool_call = select_tool(history + [user_query])\n",
        "    if (\n",
        "        tool_call is not None\n",
        "        and tool_call.name == \"get_information_for_question_answering\"\n",
        "    ):\n",
        "        print(f\"Agent: Calling tool: {tool_call.name}\")\n",
        "        # Call the tool with the arguments extracted by the LLM\n",
        "        tool_images = get_information_for_question_answering(**tool_call.args)\n",
        "        # Add images return by the tool to the list of input images if any\n",
        "        images.extend(tool_images)\n",
        "\n",
        "    # Pass the system prompt, conversation history, user query and retrieved context (`images`) to the LLM to generate an answer\n",
        "    system_prompt = f\"Answer the questions based on the provided context only. If the context is not sufficient, say I DON'T KNOW. DO NOT use any other information to answer the question.\"\n",
        "    contents = (\n",
        "        [system_prompt]\n",
        "        + history\n",
        "        + [user_query]\n",
        "        + [Image.open(image) for image in images]\n",
        "    )\n",
        "    # Get a response from the LLM\n",
        "    response = gemini_client.models.generate_content(\n",
        "        model=LLM,\n",
        "        contents=contents,\n",
        "        config=types.GenerateContentConfig(temperature=0.0),\n",
        "    )\n",
        "    answer = response.text\n",
        "    # Write the current user query to memory using the `store_chat_message` function\n",
        "    # The `role` for user queries is \"user\" and `type` is \"text\"\n",
        "    <CODE_BLOCK_17>\n",
        "    # Write the filepaths of input/retrieved images to memory using the store_chat_message` function\n",
        "    # The `role` for these is \"user\" and `type` is \"image\"\n",
        "    for image in images:\n",
        "        <CODE_BLOCK_18>\n",
        "    # Write the LLM generated response to memory\n",
        "    # The `role` for these is \"agent\" and `type` is \"text\"\n",
        "    <CODE_BLOCK_19>\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_agent(session_id: str, user_query: str, images: List = []) -> None:\n",
        "    \"\"\"\n",
        "    Execute the agent.\n",
        "\n",
        "    Args:\n",
        "        session_id (str): Session ID\n",
        "        user_query (str): User query\n",
        "        images (List, optional): List of filepaths. Defaults to [].\n",
        "    \"\"\"\n",
        "    response = generate_answer(session_id, user_query, images)\n",
        "    print(\"Agent:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "execute_agent(\n",
        "    \"1\",\n",
        "    \"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Follow-up question to make sure chat history is being used.\n",
        "execute_agent(\n",
        "    \"1\",\n",
        "    \"What did I just ask you?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🦸‍♀️ Update to ReAct agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_answer(user_query: str, images: List = []) -> str:\n",
        "    \"\"\"\n",
        "    Implement a ReAct agent\n",
        "\n",
        "    Args:\n",
        "        user_query (str): User's query string\n",
        "        images (List): List of filepaths. Defaults to [].\n",
        "\n",
        "    Returns:\n",
        "        str: LLM-generated response\n",
        "    \"\"\"\n",
        "    # Define reasoning prompt\n",
        "    system_prompt = [\n",
        "        (\n",
        "            \"You are an AI assistant. Based on the current information, decide if you have enough to answer the user query, or if you need more information.\"\n",
        "            \"If you have enough information, respond with 'ANSWER: <your answer>'.\"\n",
        "            \"If you need more information, respond with 'TOOL: <question for the tool>'. Keep the question concise.\"\n",
        "            f\"User query: {user_query}\\n\"\n",
        "            \"Current information:\\n\"\n",
        "        )\n",
        "    ]\n",
        "    # Set max iterations\n",
        "    max_iterations = 3\n",
        "    current_iteration = 0\n",
        "    # Initialize list to accumulate tool outcomes etc.\n",
        "    current_information = []\n",
        "\n",
        "    # If the user input has images, add them to `current_information`\n",
        "    if len(images) != 0:\n",
        "        current_information.extend([Image.open(image) for image in images])\n",
        "\n",
        "    # Run the reasoning -> action taking loop for `max_iterations` number of iterations\n",
        "    while current_iteration < max_iterations:\n",
        "        current_iteration += 1\n",
        "        print(f\"Iteration {current_iteration}:\")\n",
        "        # Generate action -> final answer/tool call\n",
        "        response = gemini_client.models.generate_content(\n",
        "            model=LLM,\n",
        "            contents=system_prompt + current_information,\n",
        "            config=types.GenerateContentConfig(temperature=0.0),\n",
        "        )\n",
        "        answer = response.text\n",
        "        print(f\"Agent: {answer}\")\n",
        "        # If the agent has the final answer, return it\n",
        "        if \"ANSWER\" in answer:\n",
        "            return answer\n",
        "        # If the agent decides to call a tool\n",
        "        else:\n",
        "            # determine which tool to call\n",
        "            tool_call = select_tool([answer])\n",
        "            if (\n",
        "                tool_call is not None\n",
        "                and tool_call.name == \"get_information_for_question_answering\"\n",
        "            ):\n",
        "                print(f\"Agent: Calling tool: {tool_call.name}\")\n",
        "                # Call the tool with the arguments extracted by the LLM\n",
        "                tool_images = get_information_for_question_answering(**tool_call.args)\n",
        "                # Add images return by the tool to the list of input images if any\n",
        "                current_information.extend([Image.open(image) for image in tool_images])\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_agent(user_query: str, images: List = []) -> None:\n",
        "    \"\"\"\n",
        "    Execute the agent.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): User query\n",
        "        images (List, optional): List of filepaths. Defaults to [].\n",
        "    \"\"\"\n",
        "    response = generate_answer(user_query, images)\n",
        "    print(\"Agent:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "execute_agent(\"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "execute_agent(\"Explain the graph in this image:\", [\"data/test.png\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "RM8rg08YhqZe",
        "UUf3jtFzO4-V",
        "Sm5QZdshwJLN"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09dcf4ce88064f11980bbefaad1ebc75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39563df9477648398456675ec51075aa",
            "placeholder": "​",
            "style": "IPY_MODEL_f4353368efbd4c3891f805ddc3d05e1b",
            "value": "Downloading data: 100%"
          }
        },
        "164d16df28d24ab796b7c9cf85174800": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95e4af5b420242b7a6b74a18cad98961",
            "placeholder": "​",
            "style": "IPY_MODEL_dff65b579f0746ffae8739ecb0aa5a41",
            "value": "Generating train split: "
          }
        },
        "20d693a09c534414a5c4c0dd58cf94ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "278513c5a8b04a24b1823d38107f1e50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62e196b6d30746578e137c50b661f946",
            "placeholder": "​",
            "style": "IPY_MODEL_ced7f9d61e06442a960dcda95852048e",
            "value": " 102M/102M [00:06&lt;00:00, 20.6MB/s]"
          }
        },
        "30fe0bcd02cb47f3ba23bb480e2eaaea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "373ed3b6307741859ab297c270cf42c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39563df9477648398456675ec51075aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41056c822b9d44559147d2b21416b956": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a43c349d171e469c8cc94d48060f775b",
            "placeholder": "​",
            "style": "IPY_MODEL_373ed3b6307741859ab297c270cf42c8",
            "value": " 50000/0 [00:04&lt;00:00, 12390.43 examples/s]"
          }
        },
        "62e196b6d30746578e137c50b661f946": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dbfebff68ff45628da832fac5233c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_164d16df28d24ab796b7c9cf85174800",
              "IPY_MODEL_e70e0d317f1e4e73bd95349ed1510cce",
              "IPY_MODEL_41056c822b9d44559147d2b21416b956"
            ],
            "layout": "IPY_MODEL_b1929fb112174c0abcd8004f6be0f880"
          }
        },
        "95e4af5b420242b7a6b74a18cad98961": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a43c349d171e469c8cc94d48060f775b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1929fb112174c0abcd8004f6be0f880": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cebfba144ba6418092df949783f93455": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09dcf4ce88064f11980bbefaad1ebc75",
              "IPY_MODEL_f2bd7bda4d0c4d93b88e53aeb4e1b62d",
              "IPY_MODEL_278513c5a8b04a24b1823d38107f1e50"
            ],
            "layout": "IPY_MODEL_d3941c633788427abb858b21e285088f"
          }
        },
        "ced7f9d61e06442a960dcda95852048e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d17d8c8f45ee44cd87dcd787c05dbdc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3941c633788427abb858b21e285088f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dff65b579f0746ffae8739ecb0aa5a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e70e0d317f1e4e73bd95349ed1510cce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f73ae771c24645c79fd41409a8fc7b34",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20d693a09c534414a5c4c0dd58cf94ed",
            "value": 1
          }
        },
        "f2bd7bda4d0c4d93b88e53aeb4e1b62d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30fe0bcd02cb47f3ba23bb480e2eaaea",
            "max": 102202622,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d17d8c8f45ee44cd87dcd787c05dbdc3",
            "value": 102202622
          }
        },
        "f4353368efbd4c3891f805ddc3d05e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f73ae771c24645c79fd41409a8fc7b34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "state": {}
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
